id,cn_abstract
1,"随着扩散模型（Diffusion Models, DMs）的快速发展，越来越多的工作致力于从预训练的 DMs 中“忘记”（unlearn）有害或受版权保护的概念，以防止模型被滥用。然而，观察到即使在发布前对 DMs 进行了恰当的忘记处理，恶意微调（malicious finetuning）仍然可能破坏这一过程，使 DMs 重新学习被忘记的概念。这部分是因为某些保留在 DMs 中的良性概念（例如 “skin”）与被忘记的概念（例如 “nudity”）存在关联，微调时能够促进这些概念的重新学习。为了解决此问题，我们提出了对 DMs 的元忘记（meta-unlearning）方法。直观上，元忘记后的 DM 在直接使用时应表现得如同已忘记的 DM；此外，如果对元忘记后的 DM 进行针对被忘记概念的恶意微调，内部保留的相关良性概念将被触发自毁（self-destruct），从而阻碍被忘记概念的重新学习。我们的元忘记框架兼容大多数现有的忘记方法，仅需额外加入一个易于实现的元目标（meta objective）。我们在 Stable Diffusion 模型（SD‑v1‑4 和 SDXL）上进行概念元忘记的实证实验，并通过大量消融研究（ablation studies）验证了该方法的有效性。"
2,"对 AI 生成图像进行隐形水印可用于版权保护，实现对 AI 生成媒体的检测与识别。本文提出了一种针对文本到图像（T2I）潜在扩散模型（Latent Diffusion Models，LDMs）图像的全新水印方法。仅通过微调文本标记嵌入 \(\mathcal W_*\)，即可在图像的特定对象或局部区域实现水印，相较于传统的全图水印提供了更大的灵活性。该方法利用文本编码器在不同 LDMs 之间的兼容性，实现了对各类 LDM 的即插即用（plug-and-play）集成。此外，在编码阶段早期引入水印可提升对后续流水线中对抗扰动的鲁棒性。实验表明，本文方法在仅使用约 \(10^5\) 倍模型参数缩减的情况下，能够实现 99% 的比特准确率（48 bits），从而实现高效的水印嵌入。"
3,"随着文本到图像扩散模型（text-to-image diffusion models）在商业领域的广泛应用，关于其不道德或有害使用的担忧日益增加，包括未经授权生成受版权保护或敏感内容。概念消除（Concept unlearning）已成为应对这些挑战的有前景的解决方案，旨在从预训练模型中移除不期望的有害信息。然而，之前的评估主要关注是否成功删除目标概念并保持图像质量，忽视了更广泛的影响，如意外的副作用。  在本工作中，我们提出了全方位消除基准（Holistic Unlearning Benchmark，HUB），这是一个覆盖六个关键维度的综合评估框架：忠实度（faithfulness）、对齐度（alignment）、精准度（pinpoint-ness）、多语言鲁棒性（multilingual robustness）、攻击鲁棒性（attack robustness）以及效率（efficiency）。我们的基准涵盖了33个目标概念，每个概念配备约16,000条提示，涉及四大类别：名人（Celebrity）、风格（Style）、知识产权（Intellectual Property）以及不安全内容（NSFW）。  研究结果表明，尚不存在在所有评估指标上均表现出色的单一方法。通过公开我们的评估代码和数据集，我们期望能够激励该领域的进一步研究，推动更可靠、更有效的概念消除方法的发展。"
4,"机器遗忘（Machine Unlearning, MU）旨在有选择地从模型中擦除有害行为，同时保持模型的整体效用。作为一种多任务学习（multi-task learning）问题，MU 需要在遗忘特定概念/数据的目标与保持通用性能的目标之间取得平衡。对这些遗忘与保持目标的朴素整合往往会导致梯度冲突（gradient conflicts）和支配效应（dominance），从而阻碍 MU 算法达到最优解。  为了解决梯度冲突和支配问题，我们将 MU 重新表述为一个双玩家合作博弈（two-player cooperative game），其中遗忘玩家（forgetting player）和保持玩家（preservation player）通过各自的梯度提议（gradient proposals）共同最大化整体收益并平衡彼此贡献。受 Nash 博弈论（Nash bargaining theory）启发，我们推导出闭式解（closed-form solution），引导模型趋向帕累托驻点（Pareto stationary point）。  我们的 MU 形式化保证了一个均衡解（equilibrium solution），即任何偏离最终状态的行为都会导致两位玩家的整体目标下降，从而确保每个目标的最优性。我们在图像分类和图像生成等多样化任务上评估了该算法的有效性。大量基于 ResNet、视觉语言模型 CLIP 以及文本到图像扩散模型（text-to-image diffusion models）的实验表明，我们的方法优于现有最先进的 MU 算法，在遗忘与保持之间实现了更佳的权衡。实验结果还凸显了遗忘精度（forgetting precision）的提升、通用化能力（preservation of generalization）的保持以及对对抗攻击（adversarial attacks）的鲁棒性（robustness）的增强。"
5,"Vision-Language Models (VLMs) 通过引入视觉信息扩展了 Large Language Models (LLMs) 的能力，但在处理噪声或受损图像时仍易受到 jailbreak 攻击的威胁。虽然现有 VLM 在训练阶段已采用安全措施以缓解此类攻击，但对噪声增强视觉输入的脆弱性却被忽视。本文发现，缺乏噪声增强训练导致了关键的安全漏洞：许多 VLM 对诸如 Gaussian noise 等简单扰动也极其敏感。为了解决这一问题，我们提出了 Robust-VLGuard——一个包含对齐 / 不对齐 图文对的多模态安全数据集，并结合噪声增强的微调方法，在降低攻击成功率的同时保持 VLM 的功能性。针对更强的基于优化的视觉扰动攻击，我们进一步提出 DiffPure-VLM，利用 diffusion models 将对抗扰动转换为类似 Gaussian 的噪声，从而可以通过噪声增强的安全微调来防御。实验结果表明，diffusion model 的分布迁移特性与我们微调后的 VLM 高度匹配，能够显著削弱不同强度的对抗扰动。数据集和代码已在 https://github.com/JarvisUSTC/DiffPure-RobustVLM 开源。"
6,"文本到图像（Text-to-image，T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著成功。然而，这类模型能够存储海量知识，这在需要选择性遗忘的场景中会引发问题，例如删除受版权保护的内容、降低偏见或消除有害概念。现有的遗忘方法虽然能够去除某些概念，但在多概念遗忘时仍面临不稳定、残留知识难以彻底消除以及生成质量下降等困难。为了解决这些挑战，我们提出了**Dynamic Mask coupled with Concept‑Aware Loss**，一种面向扩散模型的多概念遗忘新框架。我们的**Dynamic Mask**机制能够依据当前优化状态自适应更新梯度掩码（gradient masks），实现对权重的选择性修改，从而避免对无关知识产生干扰。此外，**Concept‑Aware Loss**通过超类对齐（superclass alignment）强制语义一致性，显式引导遗忘过程；同时基于知识蒸馏（knowledge distillation）的正则化损失确保在顺序遗忘过程中已遗忘的概念仍保持被遗忘。我们进行了大量实验以评估该方法。实验结果表明，在遗忘效果、输出保真度和语义连贯性方面，尤其是在多概念场景下，我们的方法均优于现有遗忘技术。我们的工作为生成模型提供了一个原理清晰、灵活且能够实现稳定高保真遗忘的框架。代码已在 https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025 开源。"
7,"扩散模型 (diffusion models) 已显著推动了图像合成领域的发展，使其知识产权 (IP) 的保护成为亟待解决的关键问题。现有的 IP 保护方法主要通过改变扩散过程的结构，将水印 (watermark) 嵌入生成图像中。然而，这些方法不可避免地会削弱生成图像的质量，并且在面对微调攻击（尤其是针对开源模型如 Stable Diffusion (SD)）时表现脆弱。本文提出了 PlugMark，一种新颖的插件式零水印 (zero‑watermarking) 框架用于扩散模型。PlugMark 的核心思想基于两个观察：分类器可以通过其决策边界 (decision boundaries) 唯一表征，扩散模型可以通过从训练数据中获取的知识唯一表示。基于此，我们引入了扩散知识提取器 (diffusion knowledge extractor)，该提取器可插入扩散模型以提取其知识并输出分类结果。随后，PlugMark 根据该分类结果生成边界表示 (boundary representations)，作为一种零失真水印 (zero‑distortion watermark)，唯一地代表决策边界，进而代表扩散模型的知识。由于仅需对提取器进行训练，原始扩散模型的性能保持不受影响。大量实验结果表明，PlugMark 能够稳健地从原始模型及其后处理版本中提取高置信度的零水印，并能够有效地区分未经过后处理的扩散模型。"
8,"机器学习遗忘（Machine unlearning，MU）通过从深度学习模型（deep learning models）中移除特定数据点或概念，以提升隐私保护并防止生成敏感内容。对抗提示（adversarial prompts）能够利用已遗忘的模型（unlearned models）生成包含被移除概念的内容，构成显著的安全风险。然而，现有的对抗攻击方法在生成符合攻击者意图的内容时仍面临挑战，并且在寻找成功提示时计算成本高昂。为了解决这些问题，我们提出了 ZIUM（Zero-shot Intent-aware adversarial attack on Unlearned Models），一种零样本意图感知对抗攻击框架，能够灵活定制目标攻击图像以体现攻击者的意图。此外，ZIUM 支持零样本对抗攻击（zero-shot adversarial attacks），无需对已攻击过的未学习概念进行额外优化。对多种 MU 场景的评估表明，ZIUM 在基于用户意图提示成功定制内容方面表现出色，且相较于现有方法实现了更高的攻击成功率（attack success rate）。同时，其零样本对抗攻击显著降低了对已攻击过的未学习概念的攻击时间。"
9,"针对视觉语言模型（VLMs）的去学习方法主要借鉴了大语言模型（LLMs）的技术，依赖于需要大量标注忘记集的权重更新。此外，这些方法在粗粒度上进行去学习，往往导致过度遗忘并降低模型效用。为了解决上述问题，我们提出了 SAUCE——一种利用稀疏自编码器（SAEs）实现细粒度、选择性概念去学习的新方法。简要而言，SAUCE 首先训练 SAEs，以捕获高维、语义丰富的稀疏特征；随后识别与待去学习目标概念最相关的特征。在推理阶段，SAUCE 有选择地修改这些特征，以抑制特定概念，同时保留无关信息。我们在两种不同的 VLM 上进行评估：LLaVA‑v1.5‑7B 和 LLaMA‑3.2‑11B‑Vision‑Instruct，覆盖两类任务——具体概念去学习（对象与体育场景）和抽象概念去学习（情感、颜色与材料），共计 60 个概念。大量实验表明，SAUCE 在去学习质量上比最先进的方法提升了 18.04%，且保持了相当的模型效用。此外，我们还考察了 SAUCE 对常用对抗攻击的鲁棒性、跨模型的可迁移性以及在处理多个同步去学习请求时的可扩展性。研究结果表明，SAUCE 是一种高效且可扩展的 VLM 选择性概念去学习解决方案。"
10,"虽然图像条件扩散模型（image conditional diffusion models）展示了令人印象深刻的生成能力，但在面对后门攻击（backdoor）和对抗攻击（adversarial attacks）时表现出高度脆弱性。本文定义了一种称为扩散异常（diffusion anomaly）的情景，即在受到攻击的逆过程（reverse process）中生成的结果与正常结果出现显著偏差。通过分析扩散异常的形成机制，我们揭示了扰动（perturbations）在逆过程中的放大方式以及在结果中的累积效应。基于此分析，我们发现了“发散”和“同质化”两种现象，这两者导致扩散过程显著偏离正常过程并使多样性下降。利用这两种现象，我们提出了一种名为扩散异常检测（Diffusion Anomaly Detection，DADet）的方法，能够有效检测后门攻击和对抗攻击。大量实验表明，我们的方案在防御后门攻击和对抗攻击方面取得了卓越的性能。具体而言，在后门攻击检测方面，我们的方法在包括 MS COCO 和 CIFAR-10 在内的不同数据集上实现了 99% 的 F1 分数；在对抗样本检测方面，F1 分数在三种对抗攻击和两种不同任务上均超过 84%，分别在 MS COCO 和 Places365 数据集上进行评估。"
11,"近期，文本到图像扩散模型（text-to-image diffusion models）的进展使得光照真实感图像生成成为可能，但同时也带来了生成恶意内容（如 NSFW 图像）的风险。为降低此类风险，研究者们开始探索概念擦除（concept erasure）方法，以促使模型“忘记”特定概念。然而，现有研究在完全擦除隐式嵌入于提示中的恶意概念（例如隐喻表达或对抗性提示）时仍存在困难，同时又难以保持模型的正常生成能力。为了解决这一挑战，本文提出了 TRCE，采用两阶段概念擦除策略，实现可靠擦除与知识保留之间的有效平衡。  首先，TRCE 从擦除文本提示中隐式嵌入的恶意语义入手。通过识别关键映射目标（即 [EoT] embedding），我们对交叉注意力层（cross‑attention layers）进行优化，使恶意提示映射为在语义上相似但包含安全概念的提示。此步骤可防止模型在去噪过程中过度受恶意语义的影响。  随后，考虑到扩散模型采样轨迹的确定性特征，TRCE 进一步通过对比学习（contrastive learning）将早期去噪预测引导向安全方向，远离不安全方向，从而进一步避免生成恶意内容。  最后，我们在多个恶意概念擦除基准（malicious concept erasure benchmarks）上对 TRCE 进行全面评估，实验结果表明该方法在擦除恶意概念的同时，更好地保留了模型原有的生成能力。"
12,"生成模型已迅速发展至能够产生逼真输出。然而，其合成结果日益模糊了自然内容与 AI 生成内容的界限，因而亟需稳健的 **watermarking**（水印）技术来标记合成图像。理想的水印应保持目标图像的完整性，抵御去除尝试，并防止将水印图案未经授权地插入无关图像。为满足这一需求，近期方法利用 **diffusion models**（扩散模型）过程的初始噪声在生成图像中嵌入持久水印。但这些方法要么扭曲生成图像的分布，要么在检测时必须在庞大的候选噪声模式字典中搜索。  本文提出一种新颖的水印方法：将关于生成图像的语义信息嵌入噪声模式，从而实现无失真的水印，并且无需维护关键模式数据库即可进行验证。关键模式可通过 **locality-sensitive hashing**（局部敏感哈希）从图像的 **semantic embedding**（语义嵌入）中推断得到。此外，将水印检测条件化于原始图像内容，可提升其对伪造攻击的鲁棒性。为此，我们考察了两类常被忽视的攻击策略：(i) 攻击者提取初始噪声并利用相同模式生成新图像；(ii) 攻击者在带水印的图像中插入无关（可能有害）的对象，同时保持水印完整。实验结果表明，我们的方法在上述攻击下具备更高的鲁棒性。综合来看，内容感知的水印能够在一定程度上降低图像生成模型带来的风险。"
13,"随着“被遗忘权”需求的日益增长，机器擦除（machine unlearning，MU）已成为提升信任度和合规性的关键手段，能够从机器学习（machine learning，ML）模型中移除敏感数据的影响。然而，大多数 MU 算法主要依赖于训练期间的权重调整方法，对数据层面调整在擦除过程中的潜在优势探索有限。为弥补这一空白，本文提出一种利用数字水印（digital watermarking）通过有策略地修改数据内容来促进 MU 的新方法。通过引入水印，我们构建了受控的擦除机制，实现对指定数据的精确移除，同时保持模型在无关任务上的效用。我们首先考察了带水印数据对 MU 的影响，发现 MU 能够有效地推广到带水印的数据上。基于此，本文提出了一种友好擦除的水印框架，称为 Water4MU，以提升擦除效果。Water4MU 的核心是双层优化（bi-level optimization，BLO）框架：上层优化水印网络，使其最小化擦除难度；下层则独立于水印对模型本身进行训练。实验结果表明，Water4MU 在图像分类和图像生成任务中均能有效实现 MU，且在所谓的“challenging forgets”（挑战性擦除）场景下显著优于现有方法。"
14,"视觉语言模型（Vision-Language Models, VLMs）的数据需求从早期的数百万级不断扩大到如今的数十亿级，这与数据质量之间形成了难以调和的权衡，并不可避免地引入了噪声对应（Noisy Correspondence, NC）样本。毫无疑问，这类语义不相关的数据会显著削弱 VLMs 的性能。以往的工作主要通过估计更精细的对齐来提供更准确的指导，但这些从头训练的资源密集型流水线难以满足实际的数据需求。本文提出一种全新的视角，直接在已预训练的 VLMs 中消除 NC 的有害影响。具体而言，我们提出了 NCU（Noisy Correspondence Unlearning）微调框架，通过遗忘已学习的噪声知识，高效提升 VLMs 的鲁棒性。NCU 的核心在于学习最困难的负样本信息，它能够为假阳性和假阴性提供明确的遗忘方向。该双目标遗忘过程可形式化为统一的最优传输（optimal transport）目标，从而实现快速微调。我们在主流的 CLIP 模型上对多种下游任务进行验证，结果表明，NCU 在零样本迁移（zero-shot transfer）上超越了已有的鲁棒预训练方法，并且计算开销更低。代码已公开：https://github.com/hhc1997/NCU。"
15,"生成图像水印（Generative image watermarking）实现了对生成图像的主动检测和可追溯性。在现有方法中，基于逆向的框架通过在扩散过程之前将水印注入潜在表示（latent representation）来实现高度隐蔽的水印嵌入。这种方法的鲁棒性依赖于嵌入机制（embedding mechanism）和逆向精度（inversion accuracy）。然而，以往工作主要聚焦于优化嵌入过程，忽视了逆向误差，而逆向误差会显著影响提取的保真度（extraction fidelity）。本文针对逆向误差这一挑战，提出了 ROAR——一种基于双域优化的框架，旨在减轻来自两个关键来源的误差：   1）潜在域误差（latent‑domain errors），由于固有的近似假设，在逆向步骤中累积；   2）像素域误差（pixel‑domain errors），源自 JPEG 压缩等通道失真（channel distortions）。    为解决上述问题，我们引入了两个新颖组件：   - 再生成优化机制（Regeneration‑based Optimization，RO），通过可优化的起始潜在向量（optimizable starting latent）来最小化潜在域误差；   - 基于专家混合（Mixture of Experts，MoE）的失真自适应恢复网络（distortion‑adaptive restoration，AR），能够有效从像素级失真中恢复水印分布（watermarked distributions）。    大量实验表明，ROAR 显著降低了逆向误差，提升了水印提取的鲁棒性，从而提高了生成图像水印的可靠性。"
16,"扩散模型已被证明是强大的表征学习器，在多个领域展示了最先进的性能。除了加速采样之外，DDIM（Denoising Diffusion Implicit Models）还能够将真实图像逆向映射回其潜在编码（latent codes）。该逆向操作的直接继承应用是真实图像编辑，其中逆向过程产生的潜在轨迹（latent trajectories）可在编辑图像的合成过程中使用。然而，这一实用工具也使恶意用户能够更轻松地合成误导性或深度伪造（deepfake）内容，从而促进了不道德、滥用以及侵犯隐私和版权的内容传播。虽然防御算法如 AdvDM 和 Photoguard 已被证明能够扰乱这些图像的扩散过程，但它们的目标与测试时的迭代去噪轨迹之间的不匹配导致了扰乱效果较弱。在本工作中，我们提出了 DDIM 逆向攻击（DDIM Inversion Attack，简称 DIA），针对集成的 DDIM 轨迹路径进行攻击。实验结果表明，该方法能够有效扰乱编辑过程，性能超越了此前的防御方法，适用于多种编辑技术。我们相信，我们的框架和结果能够为业界和学术界提供针对 AI 恶意使用的实用防御手段。我们的代码已公开发布： https://anonymous.4open.science/r/DIA-13419/。"
17,"不可感知的数字水印在版权保护、误信息防范以及负责任的生成式 AI 中具有重要意义。我们提出了 TrustMark——一种利用时空频谱损失函数（spatio-spectral loss function）和 1×1 卷积层（1x1 convolution layer）来提升编码质量的水印方法。TrustMark 对原位扰动（in-place perturbations）和非原位扰动（out-of-place perturbations）均表现出鲁棒性，同时保持图像质量在 43 dB 以上。此外，我们还提出了 ReMark，这是一种面向重新水印的水印去除方法，并设计了一种简洁而有效的算法，使 TrustMark 与 ReMark 能够在任意分辨率下工作。我们的技术在三个基准测试上实现了业界领先（state-of-art）的性能。"
18,"分数式生成模型（Score-based Generative Models，SGMs）展现了卓越的泛化能力，例如能够生成未见但自然的数据。然而，泛化能力越强，意外泛化的可能性越大，滥用的风险也越高。尽管存在这些担忧，针对 SGMs 的“忘记”（unlearning）研究尚未涉足。为填补这一空白，本文首先审视了机器忘记（Machine Unlearning，MU）领域的“黄金标准”——在删除不良训练数据后重新训练模型，结果发现该方法在 SGMs 中失效。对分数函数（score functions）的进一步分析表明，MU 的“黄金标准”并未改变原始的分数函数，这解释了其无效性。基于此洞察，本文提出了首个**调节式分数式生成模型**（Moderated Score-based Generative Model，MSGM），该模型引入了一种新颖的分数调整策略，在连续时间随机微分方程（continuous-time stochastic differential equation）过程中将分数函数从不良数据上“引导”开。虽然 MSGM 专为 SGMs 设计，但它是一个通用且灵活的 MU 框架，能够兼容多种扩散架构（diffusion architectures）、训练策略以及下游任务（downstream tasks）。代码已公开于 https://github.com/yunfengdiao/Moderated-Score-based-Generative-Model。"
19,"随着多模态大型语言模型（Multi-modal Large Language Models, MLLMs）的快速发展，近期出现了若干诊断基准用于评估这些模型的多模态推理能力。然而，这些基准主要侧重于视觉方面的评估，未能考察整体的音视频（audio-visual, AV）理解能力。此外，目前尚缺乏针对音视频大型语言模型（AVLLMs）在面对扰动输入时校准其响应能力的基准。为此，我们推出了音视频可信度评估基准（Audio-Visual Trustworthiness assessment Benchmark, AVTrustBench），该基准包含 60 万条样本，覆盖 9 项精心设计的任务，分别从对抗攻击（Adversarial Attack）、组合推理（Compositional Reasoning）和模态特定依赖（Modality-specific Dependency）三个维度评估 AVLLMs 的能力。基于该基准，我们对 16 种最先进的 AVLLMs 进行了广泛评测。结果表明，大多数现有模型距离人类水平的理解仍有显著差距，为未来研究提供了重要参考。为克服现有方法的局限性，我们进一步提出了一种鲁棒、模型无关的校准音视频偏好优化训练策略（robust, model agnostic calibrated audio-visual preference optimization based training strategy, CAVPref），在全部 9 项任务上实现了最高 30.19% 的性能提升。"
