id,cn_title,cn_abstract
1,"Meta-Unlearning（元反学习）在扩散模型中的应用：防止重新学习已被遗忘的概念","随着扩散模型（Diffusion Models，DMs）的快速发展，研究者们正投入大量精力从预训练的DM中去除有害或受版权保护的概念，以防止模型被滥用。然而，观察到即使在发布前对DM进行了恰当的去学习（unlearning），恶意微调（malicious finetuning）仍可能破坏这一过程，使DM重新学习被去除的概念。这部分是因为DM中保留的一些良性概念（例如“skin”）与被去除的概念（例如“nudity”）存在关联，微调时会促使这些关联的概念被重新学习。为了解决此问题，我们提出了对DM的元学习去学习（meta-unlearning）方法。直观上，元学习去学习后的DM在直接使用时应表现得如同已去除概念的DM；此外，如果对元学习去学习后的DM进行针对被去除概念的恶意微调，内部保留的相关良性概念将被触发自毁（self-destruct），从而阻碍被去除概念的重新学习。我们的元学习去学习框架兼容大多数现有的去学习方法，仅需额外加入一个易于实现的元目标（meta objective）。我们在Stable Diffusion模型（SD‑v1‑4 和 SDXL）上进行概念的元学习去学习实验，并通过大量消融实验（ablation studies）验证了该方法的有效性。"
2,"您的文本编码器（Text Encoder）可以作为对象级水印控制器（Object-Level Watermarking Controller）。","隐形水印技术用于 AI 生成图像的版权保护，可实现对 AI 生成媒体的检测与识别。本文提出了一种针对文本到图像（T2I）潜在扩散模型（Latent Diffusion Models，LDMs）图像的全新水印方法。仅通过微调文本标记嵌入 \(\mathcal{W}_*\)，即可在图像的特定对象或局部区域实现水印，相较于传统的全图水印提供了更大的灵活性。该方法利用文本编码器在不同 LDMs 之间的兼容性，实现了对各类 LDM 的即插即用集成。此外，在编码阶段早期引入水印可提升对后续流水线中对抗扰动的鲁棒性。实验表明，本文方法在仅使用约 \(10^5\) 倍参数量缩减的情况下，能够实现 99% 的比特准确率（48 位），从而实现高效的水印嵌入。"
3,"全局消除基准（Holistic Unlearning Benchmark）：面向文本到图像扩散模型（Text-to-Image Diffusion Model）消除的多维评估","随着文本到图像扩散模型（text-to-image diffusion models）在商业领域的广泛应用，关于其不道德或有害使用的担忧日益增加，包括未经授权生成受版权保护或敏感内容。概念消除（concept unlearning）已成为应对这些挑战的有前景方案，通过从预训练模型中移除不期望的有害信息。然而，以往的评估主要关注是否成功删除目标概念并保持图像质量，忽视了更广泛的影响，如意外的副作用。  在本工作中，我们提出了整体消除基准（Holistic Unlearning Benchmark，HUB），一个覆盖六个关键维度的综合评估框架：忠实度（faithfulness）、对齐度（alignment）、精准度（pinpoint-ness）、多语言鲁棒性（multilingual robustness）、攻击鲁棒性（attack robustness）以及效率（efficiency）。我们的基准涵盖了33个目标概念，每个概念对应约16,000条提示（prompts），并分为四大类：名人（Celebrity）、风格（Style）、知识产权（Intellectual Property）以及不安全内容（NSFW）。  研究结果表明，尚不存在在所有评估指标上均表现优异的单一方法。通过公开我们的评估代码和数据集，期望能够激发该领域的进一步研究，推动更可靠、更有效的概念消除方法的发展。"
4,"MUNBa：通过纳什议价 (Nash Bargaining) 实现机器遗忘 (Machine Unlearning)","机器学习消除（Machine Unlearning，MU）的目标是有选择地从模型中擦除有害行为，同时保持模型的整体效用。作为一个多任务学习（multi‑task learning）问题，MU 需要在忘记特定概念/数据的目标与保持整体性能的目标之间取得平衡。若对这些忘记与保持目标进行 naïve（朴素）集成，容易产生梯度冲突（gradient conflicts）和支配效应（dominance），从而阻碍 MU 算法达到最优解。  为了解决梯度冲突和支配问题，我们将 MU 重新表述为一个双玩家合作博弈（two‑player cooperative game），其中两位玩家——忘记玩家（forgetting player）和保持玩家（preservation player）——通过各自的梯度提议（gradient proposals）共同最大化整体收益并平衡彼此贡献。受 Nash 博弈理论（Nash bargaining theory）启发，我们推导出一个闭式解（closed‑form solution），用于引导模型趋向帕累托驻点（Pareto stationary point）。  我们的 MU 形式化保证了一个均衡解（equilibrium solution），即任何对最终状态的偏离都会导致两位玩家的整体目标下降，从而确保每个目标的最优性。我们在图像分类和图像生成等多样化任务上评估了该算法的有效性。大量基于 ResNet、视觉语言模型 CLIP 以及文本到图像扩散模型（text‑to‑image diffusion models）的实验表明，我们的方法优于现有最先进的 MU 算法，在忘记与保持之间实现了更好的权衡。实验结果还凸显了在忘记精度（forgetting precision）、通用化保持（preservation of generalization）以及对抗攻击鲁棒性（robustness against adversarial attacks）方面的提升。"
5,"保护 Vision-Language Models：缓解在基于扰动的攻击中对 Gaussian Noise 的脆弱性","Vision-Language Models（VLMs）通过融合视觉信息扩展了 Large Language Models（LLMs）的能力，但它们仍然容易受到 jailbreak 攻击，尤其在处理噪声或受损图像时更为脆弱。虽然现有 VLM 在训练过程中已采用安全措施以减轻此类攻击，但对噪声增强视觉输入的漏洞却被忽视。本文发现，缺乏噪声增强训练导致了关键的安全缺口：许多 VLM 对诸如 Gaussian noise 等简单扰动都极易受攻击。为了解决这一挑战，我们提出了 Robust-VLGuard——一个包含对齐 / 不对齐图文对的多模态安全数据集，并结合噪声增强的微调方法，在降低攻击成功率的同时保持 VLM 的功能性。针对更强的基于优化的视觉扰动攻击，我们进一步提出 DiffPure-VLM，利用 diffusion models 将对抗扰动转换为类似 Gaussian 的噪声，从而可以通过噪声增强的安全微调来防御。实验结果表明，扩散模型的分布迁移特性与我们微调后的 VLM 高度匹配，能够显著削弱不同强度的对抗扰动。数据集和代码已在 https://github.com/JarvisUSTC/DiffPure-RobustVLM 发布。"
6,"雕刻记忆：通过动态掩码 (Dynamic Mask) 与概念感知优化 (Concept-Aware Optimization) 实现扩散模型 (Diffusion Models) 的多概念遗忘  ","文本到图像（Text-to-image，T2I）扩散模型在根据文本提示生成高质量图像方面取得了显著成功。然而，这类模型能够存储海量知识，这在需要选择性遗忘的场景中会引发担忧，例如删除受版权保护的内容、降低偏见或消除有害概念。现有的遗忘（unlearning）方法虽能去除某些概念，但在多概念遗忘时仍面临不稳定、残留知识难以彻底消除以及生成质量下降等问题。为了解决这些挑战，我们提出了**动态掩码（Dynamic Mask）结合概念感知损失（Concept-Aware Loss）**的全新遗忘框架，专为扩散模型的多概念遗忘设计。我们的**动态掩码**机制能够根据当前优化状态自适应更新梯度掩码，实现对选定权重的有针对性修改，从而避免对无关知识产生干扰。此外，**概念感知损失**通过超类对齐（superclass alignment）强制语义一致性，显式引导遗忘过程；同时基于知识蒸馏（knowledge distillation）的正则化损失确保在顺序遗忘（sequential unlearning）过程中已遗忘的概念仍保持被遗忘。我们开展了大量实验对我们的方法进行评估。实验结果表明，在遗忘效果、输出保真度和语义连贯性方面，尤其是在多概念场景下，我们的方法均优于现有遗忘技术。我们的工作为生成模型提供了一个原理清晰、灵活可靠的高保真度遗忘框架。代码已公开于 https://github.com/coulsonlee/Sculpting-Memory-ICCV-2025。"
7,"PlugMark：一种用于扩散模型的 Plug‑in 零水印 (Zero‑Watermarking) 框架","扩散模型在图像合成领域取得了显著进展，使其知识产权（IP）的保护成为关键问题。现有的 IP 保护方法主要通过改变扩散过程的结构在生成图像中嵌入水印。然而，这些方法不可避免地会降低生成图像的质量，并且在面对微调攻击时尤其脆弱，尤其是对开源模型如 Stable Diffusion（SD）更是如此。本文提出了 PlugMark，一种新颖的插件式零水印（zero‑watermarking）框架用于扩散模型。PlugMark 的核心思想基于两个观察：分类器可以通过其决策边界（decision boundaries）唯一表征，扩散模型可以通过从训练数据中获取的知识唯一表示。基于此，我们引入了扩散知识提取器（diffusion knowledge extractor），该提取器可以插入到扩散模型中，提取模型的知识并输出分类结果。随后，PlugMark 根据该分类结果生成边界表示（boundary representations），作为一种零失真水印（zero‑distortion watermark），唯一地代表决策边界，进而代表扩散模型的知识。由于仅需对提取器进行训练，原始扩散模型的性能保持不受影响。大量实验结果表明，PlugMark 能够稳健地从原始模型及其后处理版本中提取高置信度的零水印，同时能够有效地区分未经过后处理的扩散模型。"
8,"ZIUM：Zero-Shot（零样本）Intent-Aware（意图感知）Adversarial Attack（对抗攻击） on Unlearned Models（未学习模型）","机器学习遗忘（Machine unlearning，MU）通过从深度学习模型（deep learning models）中移除特定数据点或概念，以提升隐私保护并防止生成敏感内容。对抗性提示（adversarial prompts）能够利用已遗忘的模型生成包含被移除概念的内容，构成显著的安全风险。然而，现有的对抗攻击方法在生成符合攻击者意图的内容时仍面临挑战，并且在寻找成功提示的过程中计算成本高昂。为了解决这些问题，我们提出了 ZIUM（Zero-shot Intent-aware adversarial attack on Unlearned Models），一种面向未学习模型的零样本意图感知对抗攻击方法，能够灵活定制目标攻击图像以体现攻击者的意图。此外，ZIUM 支持零样本对抗攻击，无需对已攻击的未学习概念进行额外优化。对多种 MU 场景的评估表明，ZIUM 在基于用户意图提示成功定制内容方面表现出色，攻击成功率显著高于现有方法。同时，其零样本对抗攻击显著缩短了对已攻击未学习概念的攻击时间。"
9,"SAUCE：使用稀疏自编码器（Sparse Autoencoders）的视觉-语言模型中的选择性概念消除（Selective Concept Unlearning）","针对视觉语言模型（VLMs）的去学习（unlearning）方法主要借鉴了大语言模型（LLMs）的技术，依赖于需要大量标注遗忘集合的权重更新。此外，这些方法的去学习粒度较粗，常导致过度遗忘并削弱模型效用。为了解决上述问题，我们提出了 SAUCE——一种利用稀疏自编码器（SAEs）实现细粒度、选择性概念去学习的创新方法。简要而言，SAUCE 首先训练 SAEs，以捕获高维、语义丰富的稀疏特征；随后识别与待去学习目标概念最相关的特征。在推理阶段，SAUCE 有选择地修改这些特征，以抑制特定概念的表现，同时保留无关信息。我们在两种不同的 VLM 上进行评估：LLaVA‑v1.5‑7B 和 LLaMA‑3.2‑11B‑Vision‑Instruct，覆盖两类任务——具体概念去学习（对象与体育场景）和抽象概念去学习（情感、颜色与材料），共计 60 个概念。大量实验表明，SAUCE 在去学习质量上比最先进的方法提升了 18.04%，且保持了相当的模型效用。此外，我们还考察了 SAUCE 对常用对抗攻击的鲁棒性、跨模型的可迁移性以及在处理多重同步去学习请求时的可扩展性。研究结果表明，SAUCE 是一种有效且可扩展的 VLM 选择性概念去学习解决方案。"
10,"DADet：通过扩散异常检测（Diffusion Anomaly Detection）保护图像条件扩散模型（Image Conditional Diffusion Models）免受对抗攻击和后门攻击。","图像条件扩散模型（image conditional diffusion models）虽然展示了令人印象深刻的生成能力，但在面对后门攻击（backdoor）和对抗攻击（adversarial attacks）时表现出高度脆弱性。本文定义了一种称为扩散异常（diffusion anomaly）的情景，即在受到攻击的逆过程（reverse process）中生成的结果与正常结果出现显著偏差。通过分析扩散异常的形成机制，我们揭示了扰动（perturbations）在逆过程中的放大方式以及其在结果中的累积效应。基于此分析，我们进一步指出了发散（divergence）和同质性（homogeneity）两种现象，这两者导致扩散过程显著偏离正常过程并使多样性下降。利用这两种现象，我们提出了一种名为扩散异常检测（Diffusion Anomaly Detection, DADet）的方法，能够有效检测后门攻击和对抗攻击。大量实验表明，我们的方案在防御后门攻击和对抗攻击方面取得了卓越的性能。具体而言，在后门攻击检测任务中，我们的方法在包括 MS COCO 和 CIFAR-10 在内的不同数据集上实现了 99% 的 F1 分数（F1 score）。在对抗样本检测方面，针对三种对抗攻击和两种不同任务的评估（分别在 MS COCO 和 Places365 数据集上），F1 分数超过了 84%。"
11,"TRCE：面向文本到图像扩散模型（Text-to-Image Diffusion Models）中可靠的恶意概念擦除（Malicious Concept Erasure）","近期，文本到图像的扩散模型（text-to-image diffusion models）在实现逼真图像生成方面取得了显著进展，但它们也可能产生恶意内容，例如不安全的（NSFW）图像。为降低风险，研究者开始探索概念擦除方法（concept erasure），以促使模型忘记特定概念。然而，现有研究在完全擦除隐式嵌入于提示中的恶意概念（如隐喻表达或对抗性提示）时仍面临困难，同时又要保持模型的正常生成能力。为解决这一挑战，本文提出了 TRCE，采用两阶段概念擦除策略，实现可靠擦除与知识保留之间的有效平衡。  首先，TRCE 从擦除文本提示中隐式嵌入的恶意语义入手。通过识别关键映射目标（即 [EoT] embedding），我们优化交叉注意力层（cross‑attention layers），将恶意提示映射为语义上相似但包含安全概念的提示。此步骤防止模型在去噪过程（denoising process）中受到恶意语义的过度影响。  随后，考虑到扩散模型采样轨迹的确定性特征，TRCE 进一步通过对比学习（contrastive learning）将早期去噪预测引导向安全方向，远离不安全方向，从而进一步避免生成恶意内容。  最后，我们在多个恶意概念擦除基准（malicious concept erasure benchmarks）上对 TRCE 进行全面评估，实验结果表明该方法在擦除恶意概念的同时，更好地保留了模型原有的生成能力。"
12,"SEAL：语义感知图像水印","生成模型已迅速发展至能够生成逼真的输出。然而，这些合成输出日益模糊了自然内容与 AI 生成内容的界限，因而亟需稳健的水印技术来标记合成图像。水印通常要求保持目标图像的完整性、抵御去除尝试，并防止将水印图案未经授权地插入无关图像。为满足这一需求，近期方法利用扩散模型（diffusion models）过程的初始噪声（initial noise）在生成图像中嵌入持久水印。但实现此目标的方式要么会扭曲生成图像的分布，要么需要在检测时搜索庞大的候选噪声模式字典（dictionary）。本文提出一种新颖的水印方法，将关于生成图像的语义信息（semantic information）嵌入噪声模式中，从而实现无失真水印（distortion‑free watermark），且无需依赖关键模式数据库（database of key patterns）即可进行验证。关键模式可以通过图像的语义嵌入（semantic embedding）结合局部敏感哈希（locality‑sensitive hashing）进行推断。此外，将水印检测条件化于原始图像内容（original image content）能够提升其对伪造攻击的鲁棒性。为证明这一点，我们考虑了两种长期被忽视的攻击策略：(i) 攻击者提取初始噪声并使用相同模式生成新图像；(ii) 攻击者在带水印的图像中插入无关（可能有害）的对象，同时保持水印完整。我们通过实验验证了该方法在上述攻击下的鲁棒性提升。综合来看，实验结果表明，内容感知水印（content‑aware watermarks）能够降低图像生成模型带来的风险。"
13,"隐形水印，显著收益：利用双层水印设计（Bi‑Level Watermarking Design）引导机器遗忘（Machine Unlearning）","随着“被遗忘权”需求的不断增长，机器消除（machine unlearning，MU）已成为提升信任度和满足监管合规性的重要手段，能够从机器学习（machine learning，ML）模型中去除敏感数据的影响。然而，大多数 MU 算法主要依赖于训练期间的权重调整方法，对数据层面的调节能够为消除过程带来的益处探索不足。为弥补这一空白，我们提出了一种利用数字水印（digital watermarking）通过有策略地修改数据内容来促进 MU 的新方法。通过引入水印，我们构建了受控的消除机制，使得能够精确删除指定数据，同时保持模型在无关任务上的效用。我们首先考察了带水印数据对 MU 的影响，发现 MU 能够有效地推广到带水印的数据上。基于此，我们提出了面向消除友好的水印框架 Water4MU，以提升消除效果。Water4MU 的核心是一个双层优化（bi-level optimization，BLO）框架：上层优化水印网络，使其最小化消除难度；下层则独立于水印对模型本身进行训练。实验结果表明，Water4MU 在图像分类和图像生成任务中均能有效实现 MU。值得注意的是，在被称为“challenging forgets”的困难消除场景中，它显著优于现有方法。"
14,"消除噪声对应（Noisy Correspondence）使 CLIP 更加鲁棒。","视觉语言模型 (Vision-Language Models, VLMs) 对数据的需求从早期的数百万规模不断扩大到如今的数十亿规模，这与数据质量之间形成了难以平衡的权衡，必然导致噪声对应 (Noisy Correspondence, NC) 样本的出现。显然，这类语义不相关的数据会显著削弱 VLMs 的性能。以往的工作主要通过估计更精细的对齐来提供更准确的指导，但这些从头训练的资源密集型流程难以满足实际的数据需求。本文提出一种全新的视角，直接在已预训练的 VLMs 中消除 NC 的有害影响。具体而言，我们提出了 Noisy Correspondence Unlearning (NCU) 微调框架，通过遗忘已学习的噪声知识，高效提升 VLMs 的鲁棒性。NCU 的核心在于学习最困难的负样本信息，为假阳性 (false positives) 与假阴性 (false negatives) 提供明确的“忘记”方向。该双目标遗忘过程可形式化为统一的最优传输 (optimal transport) 目标，从而实现快速微调。我们在主流的 CLIP 模型上对多个下游任务进行验证，结果表明 NCU 在零样本迁移 (zero-shot transfer) 上超越了已有的鲁棒预训练方法，同时计算开销更低。代码已公开：https://github.com/hhc1997/NCU。"
15,"ROAR：降低生成式图像水印（Generative Image Watermarking）中的反演误差（Inversion Error）","生成式图像水印（Generative image watermarking）实现了对生成图像的主动检测与可追溯性。现有方法中，基于逆向的框架通过在扩散过程之前向潜在表示（latent representation）注入水印，达到了高度隐蔽的水印嵌入。该方法的鲁棒性依赖于嵌入机制和逆向精度两方面。然而，以往工作主要聚焦于优化嵌入过程，忽视了逆向误差，而逆向误差对提取保真度影响显著。本文针对逆向误差这一挑战，提出了 ROAR——一种基于双域优化的框架，旨在缓解来自两个关键来源的误差：1）潜在域误差（latent‑domain errors），该误差在逆向步骤中因固有的近似假设而累计；2）像素域误差（pixel‑domain errors），该误差源于 JPEG 压缩等通道失真。为解决上述问题，我们引入了两个创新组件：① 再生式优化（Regeneration‑based Optimization，RO）机制，通过可优化的起始潜在向量最小化潜在域误差；② 基于专家混合（Mixture of Experts，MoE）的失真自适应恢复（distortion‑adaptive restoration，AR）网络，能够有效从像素级失真中恢复水印分布。大量实验表明，ROAR 能显著降低逆向误差并提升水印提取的鲁棒性，从而提高生成式图像水印的可靠性。"
